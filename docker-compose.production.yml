# ==================================================
# Production Docker Compose for Coolify Deployment
# ==================================================
# This configuration deploys the complete Agent Platform stack for production
# with Caddy reverse proxy labels for subdomain routing.
# 
# Features:
# - All Supabase services (database, auth, storage, etc.)
# - Application services in production mode (LangConnect, MCP Server, Web Frontend)
# - Workflow automation (n8n, Windmill)
# - Automatic database migrations
# - Caddy reverse proxy integration

# Define persistent volumes for data storage across container restarts
volumes:
  n8n_storage:              # Stores n8n workflow automation data
  langconnect_storage:      # Stores LangConnect API data  
  mcp_server_storage:       # Stores MCP server data
  windmill_db_data:         # Stores Windmill PostgreSQL data
  windmill_worker_logs:     # Shared worker logs across all Windmill workers
  windmill_worker_dependency_cache: # Shared dependency cache for efficiency
  windmill_lsp_cache:       # LSP service cache for code intelligence
  supabase-db-data:         # Supabase database data
  supabase-storage:         # Supabase storage files
  db-config:                # Database configuration volume

# Connect to Coolify's network for Caddy reverse proxy
networks:
  coolify:
    external: true

services:
  # ==========================================
  # SUPABASE CORE SERVICES
  # ==========================================
  
  # Supabase PostgreSQL Database
  db_prod:
    build:
      context: ./supabase/volumes/db
      dockerfile_inline: |
        FROM supabase/postgres:15.8.1.060
        # Copy SQL initialization files
        COPY realtime.sql /docker-entrypoint-initdb.d/migrations/99-realtime.sql
        COPY webhooks.sql /docker-entrypoint-initdb.d/init-scripts/98-webhooks.sql
        COPY roles.sql /docker-entrypoint-initdb.d/init-scripts/99-roles.sql
        COPY jwt.sql /docker-entrypoint-initdb.d/init-scripts/99-jwt.sql
        COPY _supabase.sql /docker-entrypoint-initdb.d/migrations/97-_supabase.sql
        COPY logs.sql /docker-entrypoint-initdb.d/migrations/99-logs.sql
        COPY pooler.sql /docker-entrypoint-initdb.d/migrations/99-pooler.sql
    container_name: supabase-db
    restart: unless-stopped
    networks:
      - coolify
    volumes:
      - supabase-db-data:/var/lib/postgresql/data:Z
      - db-config:/etc/postgresql-custom
    healthcheck:
      test:
        [
        "CMD-SHELL",
        "pg_isready -U postgres -h localhost && psql -U postgres -h localhost -c 'SELECT 1' > /dev/null 2>&1"
        ]
      interval: 5s
      timeout: 10s
      retries: 20
      start_period: 30s
    depends_on:
      vector_prod:
        condition: service_healthy
    environment:
      POSTGRES_HOST: /var/run/postgresql
      PGPORT: 5432
      POSTGRES_PORT: 5432
      PGPASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGDATABASE: postgres
      POSTGRES_DB: postgres
      JWT_SECRET: ${JWT_SECRET}
      JWT_EXP: 3600
    command:
      [
        "postgres",
        "-c",
        "config_file=/etc/postgresql/postgresql.conf",
        "-c",
        "log_min_messages=fatal"
      ]
  
  # Supabase Auth
  auth_prod:
    container_name: supabase-auth-prod
    image: supabase/gotrue:v2.176.1
    restart: unless-stopped
    networks:
      - coolify
    environment:
      GOTRUE_API_HOST: 0.0.0.0
      GOTRUE_API_PORT: 9999
      API_EXTERNAL_URL: ${SUPABASE_PUBLIC_URL}
      GOTRUE_DB_DRIVER: postgres
      GOTRUE_DB_DATABASE_URL: postgres://supabase_auth_admin:${POSTGRES_PASSWORD}@db_prod:5432/postgres
      GOTRUE_SITE_URL: ${FRONTEND_BASE_URL}
      GOTRUE_URI_ALLOW_LIST: "${FRONTEND_BASE_URL}/**,${FRONTEND_BASE_URL}/auth/callback"
      GOTRUE_DISABLE_SIGNUP: ${DISABLE_SIGNUP:-true}
      GOTRUE_JWT_ADMIN_ROLES: service_role
      GOTRUE_JWT_AUD: authenticated
      GOTRUE_JWT_DEFAULT_GROUP_NAME: authenticated
      GOTRUE_JWT_EXP: 3600
      GOTRUE_JWT_SECRET: ${JWT_SECRET}
      GOTRUE_EXTERNAL_EMAIL_ENABLED: ${ENABLE_EMAIL_SIGNUP:-true}
      GOTRUE_EXTERNAL_ANONYMOUS_USERS_ENABLED: ${ENABLE_ANONYMOUS_USERS:-false}
      GOTRUE_MAILER_AUTOCONFIRM: ${ENABLE_EMAIL_AUTOCONFIRM:-false}
      GOTRUE_SMTP_ADMIN_EMAIL: ${SMTP_ADMIN_EMAIL}
      GOTRUE_SMTP_HOST: ${SMTP_HOST}
      GOTRUE_SMTP_PORT: 587
      GOTRUE_SMTP_USER: ${SMTP_USER}
      GOTRUE_SMTP_PASS: ${SMTP_PASS}
      GOTRUE_SMTP_SENDER_NAME: ${SMTP_SENDER_NAME}
      GOTRUE_MAILER_URLPATHS_INVITE: /reset-password?type=invite
      GOTRUE_MAILER_URLPATHS_CONFIRMATION: /
      GOTRUE_MAILER_URLPATHS_RECOVERY: /reset-password?type=recovery
      GOTRUE_MAILER_URLPATHS_EMAIL_CHANGE: /
      GOTRUE_MAILER_SECURE_EMAIL_CHANGE_ENABLED: true
      GOTRUE_EXTERNAL_PHONE_ENABLED: ${ENABLE_PHONE_SIGNUP:-false}
      GOTRUE_SMS_AUTOCONFIRM: false
      # Email Templates (auto-generated URLs)
      GOTRUE_MAILER_TEMPLATES_INVITE: ${FRONTEND_BASE_URL}/api/email-templates/invite
      GOTRUE_MAILER_TEMPLATES_CONFIRMATION: ${FRONTEND_BASE_URL}/api/email-templates/confirmation
      GOTRUE_MAILER_TEMPLATES_RECOVERY: ${FRONTEND_BASE_URL}/api/email-templates/recovery
      GOTRUE_MAILER_TEMPLATES_MAGIC_LINK: ${FRONTEND_BASE_URL}/api/email-templates/magic-link
      GOTRUE_MAILER_TEMPLATES_EMAIL_CHANGE: ${FRONTEND_BASE_URL}/api/email-templates/email-change
      # Email Subjects
      GOTRUE_MAILER_SUBJECTS_INVITE: "Welcome to Agent Platform!"
      GOTRUE_MAILER_SUBJECTS_CONFIRMATION: "Please confirm your email - Agent Platform"
      GOTRUE_MAILER_SUBJECTS_RECOVERY: "Reset your password - Agent Platform"
      GOTRUE_MAILER_SUBJECTS_MAGIC_LINK: "Your magic sign-in link - Agent Platform"
      GOTRUE_MAILER_SUBJECTS_EMAIL_CHANGE: "Confirm your email change - Agent Platform"
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://localhost:9999/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    depends_on:
      db_prod:
        condition: service_healthy
      # analytics_prod:
      #   condition: service_healthy
  
  # PostgREST API
  rest_prod:
    container_name: supabase-rest-prod
    image: postgrest/postgrest:v12.2.12
    restart: unless-stopped
    networks:
      - coolify
    environment:
      PGRST_DB_URI: postgres://authenticator:${POSTGRES_PASSWORD}@db_prod:5432/postgres
      PGRST_DB_SCHEMAS: public,storage,graphql_public
      PGRST_DB_ANON_ROLE: anon
      PGRST_JWT_SECRET: ${JWT_SECRET}
      PGRST_DB_USE_LEGACY_GUCS: "false"
      PGRST_APP_SETTINGS_JWT_SECRET: ${JWT_SECRET}
      PGRST_APP_SETTINGS_JWT_EXP: 3600
    depends_on:
      db_prod:
        condition: service_healthy
      # analytics_prod:
      #   condition: service_healthy
  
  # Supabase Realtime
  realtime_prod:
    container_name: supabase-realtime-prod
    image: supabase/realtime:v2.34.47
    restart: unless-stopped
    networks:
      - coolify
    environment:
      PORT: 4000
      DB_HOST: db_prod
      DB_PORT: 5432
      DB_USER: supabase_admin
      DB_PASSWORD: ${POSTGRES_PASSWORD}
      DB_NAME: postgres
      DB_AFTER_CONNECT_QUERY: 'SET search_path TO _realtime'
      DB_ENC_KEY: supabaserealtime
      API_JWT_SECRET: ${JWT_SECRET}
      SECRET_KEY_BASE: ${SECRET_KEY_BASE}
      ERL_AFLAGS: -proto_dist inet_tcp
      DNS_NODES: "''"
      RLIMIT_NOFILE: "10000"
      APP_NAME: realtime
      SEED_SELF_HOST: true
      RUN_JANITOR: true
      LOG_LEVEL: info
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-sSfL",
          "--head",
          "-o",
          "/dev/null",
          "-H",
          "Authorization: Bearer ${SUPABASE_ANON_KEY}",
          "http://localhost:4000/api/tenants/realtime-dev/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    depends_on:
      db_prod:
        condition: service_healthy
      # analytics_prod:
      #   condition: service_healthy
  
  # Supabase Storage
  storage_prod:
    container_name: supabase-storage-prod
    image: supabase/storage-api:v1.24.7
    restart: unless-stopped
    networks:
      - coolify
    environment:
      ANON_KEY: ${SUPABASE_ANON_KEY}
      SERVICE_KEY: ${SUPABASE_SERVICE_KEY}
      POSTGREST_URL: http://rest_prod:3000
      PGRST_JWT_SECRET: ${JWT_SECRET}
      DATABASE_URL: postgres://supabase_storage_admin:${POSTGRES_PASSWORD}@db_prod:5432/postgres
      FILE_SIZE_LIMIT: 52428800
      STORAGE_BACKEND: file
      FILE_STORAGE_BACKEND_PATH: /var/lib/storage
      TENANT_ID: stub
      REGION: stub
      GLOBAL_S3_BUCKET: stub
      ENABLE_IMAGE_TRANSFORMATION: "true"
      IMGPROXY_URL: http://imgproxy_prod:5001
    volumes:
      - supabase-storage:/var/lib/storage:z
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://127.0.0.1:5000/status"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    depends_on:
      db_prod:
        condition: service_healthy
      rest_prod:
        condition: service_started
      imgproxy_prod:
        condition: service_started
  
  # Image Proxy for Storage
  imgproxy_prod:
    container_name: supabase-imgproxy-prod
    image: darthsim/imgproxy:v3.8.0
    restart: unless-stopped
    networks:
      - coolify
    environment:
      IMGPROXY_BIND: ":5001"
      IMGPROXY_LOCAL_FILESYSTEM_ROOT: /
      IMGPROXY_USE_ETAG: "true"
      IMGPROXY_ENABLE_WEBP_DETECTION: ${IMGPROXY_ENABLE_WEBP_DETECTION:-false}
    volumes:
      - supabase-storage:/var/lib/storage:z
    healthcheck:
      test:
        [
          "CMD",
          "imgproxy",
          "health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3

  # Supabase Analytics - DISABLED to save memory (~1.5GB per instance)
  # analytics_prod:
  #   container_name: supabase-analytics-prod
  #   image: supabase/logflare:1.14.2
  #   restart: unless-stopped
  #   networks:
  #     - coolify
  #   healthcheck:
  #     test:
  #       [
  #         "CMD",
  #         "curl",
  #         "http://localhost:4000/health"
  #       ]
  #     timeout: 5s
  #     interval: 5s
  #     retries: 10
  #   depends_on:
  #     db_prod:
  #       condition: service_healthy
  #   environment:
  #     LOGFLARE_NODE_HOST: 127.0.0.1
  #     DB_USERNAME: supabase_admin
  #     DB_DATABASE: _supabase
  #     DB_HOSTNAME: db_prod
  #     DB_PORT: 5432
  #     DB_PASSWORD: ${POSTGRES_PASSWORD}
  #     DB_SCHEMA: _analytics
  #     LOGFLARE_PUBLIC_ACCESS_TOKEN: ${LOGFLARE_PUBLIC_ACCESS_TOKEN}
  #     LOGFLARE_PRIVATE_ACCESS_TOKEN: ${LOGFLARE_PRIVATE_ACCESS_TOKEN}
  #     LOGFLARE_SINGLE_TENANT: true
  #     LOGFLARE_SUPABASE_MODE: true
  #     LOGFLARE_MIN_CLUSTER_SIZE: 1
  #     POSTGRES_BACKEND_URL: postgresql://supabase_admin:${POSTGRES_PASSWORD}@db_prod:5432/_supabase
  #     POSTGRES_BACKEND_SCHEMA: _analytics
  #     LOGFLARE_FEATURE_FLAG_OVERRIDE: multibackend=true

  # Supabase Meta (required by Studio)
  meta_prod:
    container_name: supabase-meta-prod
    image: supabase/postgres-meta:v0.89.3
    restart: unless-stopped
    networks:
      - coolify
    environment:
      PG_META_PORT: 8080
      PG_META_DB_HOST: db_prod
      PG_META_DB_PORT: 5432
      PG_META_DB_NAME: postgres
      PG_META_DB_USER: supabase_admin
      PG_META_DB_PASSWORD: ${POSTGRES_PASSWORD}
    depends_on:
      db_prod:
        condition: service_healthy
      # analytics_prod:
      #   condition: service_healthy
  
  # Kong API Gateway
  kong_prod:
    build:
      context: ./supabase/volumes/api
      dockerfile_inline: |
        FROM kong:2.8.1
        USER root
        RUN apk update && apk add --no-cache gettext
        COPY kong.prod.yml /home/kong/kong.yml.template
        USER kong
        EXPOSE 8000 8443
        ENTRYPOINT ["/bin/bash", "-c", "envsubst < /home/kong/kong.yml.template > /home/kong/kong.yml && /docker-entrypoint.sh kong docker-start"]
    container_name: supabase-kong-prod
    restart: unless-stopped
    networks:
      - coolify
    labels:
      - caddy=supabase.yourdomain.com
      - caddy.reverse_proxy={{upstreams 8000}}
    environment:
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /home/kong/kong.yml
      KONG_DNS_ORDER: LAST,A,CNAME
      KONG_PLUGINS: request-transformer,cors,key-auth,acl,basic-auth
      KONG_NGINX_PROXY_PROXY_BUFFER_SIZE: 160k
      KONG_NGINX_PROXY_PROXY_BUFFERS: 64 160k
      SUPABASE_ANON_KEY: ${SUPABASE_ANON_KEY}
      SUPABASE_SERVICE_KEY: ${SUPABASE_SERVICE_KEY}
      DASHBOARD_USERNAME: ${DASHBOARD_USERNAME}
      DASHBOARD_PASSWORD: ${DASHBOARD_PASSWORD}
    depends_on:
      db_prod:
        condition: service_healthy
      # analytics_prod:
      #   condition: service_healthy

  # Studio (Admin Interface)
  studio_prod:
    container_name: supabase-studio-prod
    image: supabase/studio:2025.06.30-sha-6f5982d
    restart: unless-stopped
    networks:
      - coolify
    environment:
      STUDIO_PG_META_URL: http://meta_prod:8080
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      DEFAULT_ORGANIZATION_NAME: "Agent Platform"
      DEFAULT_PROJECT_NAME: "Production"
      SUPABASE_URL: http://kong_prod:8000
      SUPABASE_PUBLIC_URL: ${SUPABASE_PUBLIC_URL}
      SUPABASE_ANON_KEY: ${SUPABASE_ANON_KEY}
      SUPABASE_SERVICE_KEY: ${SUPABASE_SERVICE_KEY}
      HOSTNAME: 0.0.0.0
      PORT: 3000
    depends_on:
      kong_prod:
        condition: service_started
 
  # Vector for log collection
  vector_prod:
    build:
      context: ./supabase/volumes/logs
      dockerfile_inline: |
        FROM timberio/vector:0.28.1-alpine
        COPY vector.yml /etc/vector/vector.yml
        EXPOSE 9001
        CMD ["--config", "/etc/vector/vector.yml"]
    container_name: supabase-vector-prod
    restart: unless-stopped
    networks:
      - coolify
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro,z
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://vector_prod:9001/health"
        ]
      timeout: 5s
      interval: 5s
      retries: 3
    environment:
      # LOGFLARE_PUBLIC_ACCESS_TOKEN: ${LOGFLARE_PUBLIC_ACCESS_TOKEN}  # Disabled with analytics
      DUMMY_ENV: "true"  # Placeholder since analytics is disabled

  # Supavisor (Connection Pooler)
  pooler_prod:
    build:
      context: ./supabase/volumes/pooler
      dockerfile_inline: |
        FROM supabase/supavisor:2.5.6
        COPY pooler.exs /etc/pooler/pooler.exs
        EXPOSE 4000
        CMD ["/bin/sh", "-c", "/app/bin/migrate && /app/bin/supavisor eval \"$$(cat /etc/pooler/pooler.exs)\" && /app/bin/server"]
    container_name: supabase-pooler-prod
    restart: unless-stopped
    networks:
      - coolify
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-sSfL",
          "--head",
          "-o",
          "/dev/null",
          "http://127.0.0.1:4000/api/health"
        ]
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      db_prod:
        condition: service_healthy
      # analytics_prod:
      #   condition: service_healthy
    environment:
      PORT: 4000
      POSTGRES_PORT: 5432
      POSTGRES_DB: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      DATABASE_URL: ecto://supabase_admin:${POSTGRES_PASSWORD}@db_prod:5432/_supabase
      CLUSTER_POSTGRES: true
      SECRET_KEY_BASE: ${SECRET_KEY_BASE}
      VAULT_ENC_KEY: ${VAULT_ENC_KEY}
      API_JWT_SECRET: ${JWT_SECRET}
      METRICS_JWT_SECRET: ${JWT_SECRET}
      REGION: production
      ERL_AFLAGS: -proto_dist inet_tcp
      LOG_LEVEL: info
      POOLER_TENANT_ID: ${POOLER_TENANT_ID:-1000}
      POOLER_DEFAULT_POOL_SIZE: ${POOLER_DEFAULT_POOL_SIZE:-25}
      POOLER_MAX_CLIENT_CONN: ${POOLER_MAX_CLIENT_CONN:-200}
      POOLER_POOL_MODE: transaction
      DB_POOL_SIZE: ${POOLER_DB_POOL_SIZE:-5}

  # ==========================================
  # DATABASE MIGRATION INIT CONTAINER
  # ==========================================

  # Migration Init: Runs database migrations before application services start
  migration-init:
    build:
      context: ./database
      dockerfile: Dockerfile.migration
    container_name: agent-platform-migration-init
    restart: "no"
    networks:
      - coolify
    environment:
      - POSTGRES_HOST=db_prod
      - POSTGRES_PORT=5432
      - POSTGRES_DB=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - MAX_WAIT_TIME=180
      - WAIT_RETRY_INTERVAL=5
    depends_on:
      db_prod:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "migrate.py", "--status"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # ==========================================
  # APPLICATION SERVICES (PRODUCTION)
  # ==========================================

  # LangConnect: RAG Backend API in production mode
  langconnect:
    build:
      context: ./apps/langconnect
      dockerfile: Dockerfile  # Production Dockerfile
    container_name: langconnect-prod
    restart: unless-stopped
    networks:
      coolify:
        aliases:
          - langconnect-prod
    labels:
      - caddy=langconnect.yourdomain.com
      - caddy.reverse_proxy={{upstreams 8080}}
    environment:
      # Database connection
      - LANGCONNECT_POSTGRES_HOST=db_prod
      - LANGCONNECT_POSTGRES_PORT=5432
      - LANGCONNECT_POSTGRES_USER=postgres
      - LANGCONNECT_POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - LANGCONNECT_POSTGRES_DB=postgres
      - LANGCONNECT_POSTGRES_SCHEMA=langconnect
      # LangGraph configuration
      - LANGCONNECT_LANGGRAPH_API_URL=${LANGGRAPH_EXTERNAL_URL}
      - LANGSMITH_API_KEY=${LANGSMITH_API_KEY}
      # Authentication (internal Supabase URLs)
      - SUPABASE_URL=http://kong_prod:8000
      - SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      # External API Keys
      - LANGCONNECT_SERVICE_ACCOUNT_KEY=${LANGCONNECT_SERVICE_ACCOUNT_KEY}
      - SUPADATA_API_TOKEN=${SUPADATA_API_TOKEN}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      # API configuration
      - API_HOST=0.0.0.0
      - API_PORT=8080
      - DEBUG=${DEBUG:-false}
      # Production settings
      - UVICORN_RELOAD=false
      - UVICORN_LOG_LEVEL=info
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # CORS configuration
      - ALLOW_ORIGINS=["${FRONTEND_BASE_URL}"]
      # Sentry (production)
      - SENTRY_DSN_LANGCONNECT=${SENTRY_DSN_LANGCONNECT}
      - SENTRY_ENVIRONMENT=production
      - SENTRY_TRACES_SAMPLE_RATE=${SENTRY_TRACES_SAMPLE_RATE:-0.1}
      - SENTRY_PROFILES_SAMPLE_RATE=${SENTRY_PROFILES_SAMPLE_RATE:-0.0}
    volumes:
      - langconnect_storage:/app/data
    depends_on:
      migration-init:
        condition: service_completed_successfully

  # Custom MCP Server: Model Context Protocol server in production mode
  mcp-server:
    build:
      context: ./apps/mcp
      dockerfile: Dockerfile  # Production Dockerfile
    container_name: mcp-server-prod
    restart: unless-stopped
    networks:
      - coolify
    labels:
      - caddy=mcp.yourdomain.com
      - caddy.reverse_proxy={{upstreams 8001}}
    environment:
      # Server configuration
      - MCP_SERVER_PORT=8001
      - MCP_SERVER_HOST=0.0.0.0
      - MCP_SERVER_LOG_LEVEL=${LOG_LEVEL:-INFO}
      - MCP_SERVER_LOG_NAME=custom-mcp-server
      - MCP_SERVICE_ACCOUNT_KEY=${MCP_SERVICE_ACCOUNT_KEY}
      # Authentication (internal Supabase URLs)
      - AUTH_PROVIDER=supabase
      - SUPABASE_URL=http://kong_prod:8000
      - SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      - MCP_TOKEN_SIGNING_SECRET=${MCP_TOKEN_SIGNING_SECRET}
      # External API keys
      - ARCADE_API_KEY=${ARCADE_API_KEY}
      - ARCADE_BASE_URL=https://api.arcade.dev
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - E2B_API_KEY=${E2B_API_KEY}
      - LANGCONNECT_BASE_URL=http://langconnect-prod:8080
      # MCP configuration
      - ENABLE_OAUTH_DISCOVERY=true
      - FRONTEND_BASE_URL=${FRONTEND_BASE_URL}
      - MCP_PUBLIC_BASE_URL=${MCP_PUBLIC_URL}
      - OAUTH_ISSUER=${SUPABASE_PUBLIC_URL}
      - TOOL_EXECUTION_TIMEOUT=300
      - ENABLE_CUSTOM_TOOLS=true
      - CUSTOM_TOOLS_CONFIG_PATH=./config/custom_tools.json
      - ENABLE_ARCADE=${ENABLE_ARCADE:-true}
      - ENABLED_ARCADE_SERVICES=${ENABLED_ARCADE_SERVICES}
      - TOOL_CACHE_TTL=3600
      - USER_AUTH_CACHE_TTL=1800
      # Production settings
      - DEBUG=${DEBUG:-false}
      - ENABLE_CORS=true
      - CORS_ORIGINS="*"
      - UVICORN_RELOAD=false
      - UVICORN_LOG_LEVEL=info
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # Sentry (production)
      - SENTRY_DSN_MCP=${SENTRY_DSN_MCP}
      - SENTRY_ENVIRONMENT=production
      - SENTRY_TRACES_SAMPLE_RATE=${SENTRY_TRACES_SAMPLE_RATE:-0.1}
      - SENTRY_PROFILES_SAMPLE_RATE=${SENTRY_PROFILES_SAMPLE_RATE:-0.0}
    volumes:
      - mcp_server_storage:/app/data
    depends_on:
      migration-init:
        condition: service_completed_successfully

  # Web Frontend: Next.js application in production mode
  web-frontend:
    build:
      context: ./apps/web
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_SUPABASE_URL=${SUPABASE_PUBLIC_URL}
        - NEXT_PUBLIC_SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
        - NEXT_PUBLIC_LANGCONNECT_API_URL=${LANGCONNECT_BASE_URL}
        - NEXT_PUBLIC_MCP_SERVER_URL=${MCP_PUBLIC_URL}
        - NEXT_PUBLIC_LANGGRAPH_API_URL=${LANGGRAPH_EXTERNAL_URL}
        - NEXT_PUBLIC_N8N_URL=${N8N_PUBLIC_URL}
        - NEXT_PUBLIC_WINDMILL_URL=${WINDMILL_PUBLIC_URL}
        - NEXT_PUBLIC_BASE_API_URL=${FRONTEND_BASE_URL}/api
        - NEXT_PUBLIC_LANGGRAPH_DEPLOYMENT_ID=${LANGGRAPH_DEPLOYMENT_ID}
        - NEXT_PUBLIC_LANGGRAPH_TENANT_ID=${LANGGRAPH_TENANT_ID}
        - NEXT_PUBLIC_LANGGRAPH_DEFAULT_GRAPH_ID=${LANGGRAPH_DEFAULT_GRAPH_ID}
        - NEXT_PUBLIC_USE_LANGSMITH_AUTH=false
        - NEXT_PUBLIC_GOOGLE_AUTH_DISABLED=true
        - NEXT_PUBLIC_FRONTEND_BASE_URL=${FRONTEND_BASE_URL}
    container_name: web-frontend-prod
    restart: unless-stopped
    networks:
      - coolify
    labels:
      - caddy=app.yourdomain.com
      - caddy.reverse_proxy={{upstreams 3000}}
    environment:
      - NODE_ENV=production
      - NEXT_TELEMETRY_DISABLED=1
      - MCP_TOKEN_SIGNING_SECRET=${MCP_TOKEN_SIGNING_SECRET}
      - FRONTEND_BASE_URL=${FRONTEND_BASE_URL}
      - NEXT_PUBLIC_BASE_URL=${FRONTEND_BASE_URL}
      - NEXT_PUBLIC_SUPABASE_URL=${SUPABASE_PUBLIC_URL}
      - NEXT_PUBLIC_SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
      # Internal URLs for server-side operations
      - SUPABASE_URL=http://kong_prod:8000
      - SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
      - LANGCONNECT_BASE_URL=http://langconnect-prod:8080
      - NEXT_PUBLIC_LANGCONNECT_API_URL=${LANGCONNECT_BASE_URL}
      - NEXT_PUBLIC_MCP_SERVER_URL=${MCP_PUBLIC_URL}
      - NEXT_PUBLIC_LANGGRAPH_API_URL=${LANGGRAPH_EXTERNAL_URL}
      - NEXT_PUBLIC_N8N_URL=${N8N_PUBLIC_URL}
      - NEXT_PUBLIC_WINDMILL_URL=${WINDMILL_PUBLIC_URL}
      - NEXT_PUBLIC_BASE_API_URL=${FRONTEND_BASE_URL}/api
      - NEXT_PUBLIC_LANGGRAPH_DEPLOYMENT_ID=${LANGGRAPH_DEPLOYMENT_ID}
      - NEXT_PUBLIC_LANGGRAPH_TENANT_ID=${LANGGRAPH_TENANT_ID}
      - NEXT_PUBLIC_LANGGRAPH_DEFAULT_GRAPH_ID=${LANGGRAPH_DEFAULT_GRAPH_ID}
      - NEXT_PUBLIC_USE_LANGSMITH_AUTH=false
      - NEXT_PUBLIC_GOOGLE_AUTH_DISABLED=true
      - NEXT_PUBLIC_FRONTEND_BASE_URL=${FRONTEND_BASE_URL}
      # Sentry (production)
      - SENTRY_ENVIRONMENT=production
      - SENTRY_SOURCEMAPS_DISABLE=false
      - SENTRY_TRACES_SAMPLE_RATE=${SENTRY_TRACES_SAMPLE_RATE:-0.1}
      - SENTRY_REPLAYS_SESSION_SAMPLE_RATE=${SENTRY_REPLAYS_SESSION_SAMPLE_RATE:-0.0}
      - SENTRY_REPLAYS_ON_ERROR_SAMPLE_RATE=${SENTRY_REPLAYS_ON_ERROR_SAMPLE_RATE:-1.0}
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--no-verbose",
          "--tries=1",
          "--spider",
          "http://127.0.0.1:3000/"
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      migration-init:
        condition: service_completed_successfully

  # ==========================================
  # WORKFLOW AUTOMATION SERVICES
  # ==========================================

  # n8n: Workflow automation service
  n8n-import:
    build:
      context: .
      dockerfile: n8n/import/Dockerfile
    pull_policy: always
    container_name: n8n-import-prod
    restart: "no"  # Run once and exit - don't restart
    networks:
      - coolify
    entrypoint: /bin/sh
    command:
      [
        "-c",
        "set -e; echo 'Listing /data:'; ls -la /data || true; echo 'Listing /data/credentials:'; ls -la /data/credentials || true; echo 'Listing /data/workflows:'; ls -la /data/workflows || true; \
        if ls /data/credentials/*.json >/dev/null 2>&1; then echo 'Importing credentials...'; n8n import:credentials --separate --input=/data/credentials --debug || exit 1; else echo 'No credentials to import.'; fi; \
        if ls /data/workflows/*.json >/dev/null 2>&1; then echo 'Importing workflows...'; n8n import:workflow --separate --input=/data/workflows --debug || exit 1; else echo 'No workflows to import.'; fi"
      ]
    environment:
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=db_prod
      - DB_POSTGRESDB_USER=postgres
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
      - DB_POSTGRESDB_DATABASE=postgres
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}
      - N8N_USER_MANAGEMENT_JWT_SECRET=${N8N_USER_MANAGEMENT_JWT_SECRET}
    # No volumes here on purpose: use baked-in /data from the import image
    depends_on:
      db_prod:
        condition: service_healthy

  n8n:
    image: docker.n8n.io/n8nio/n8n:latest
    pull_policy: always
    container_name: n8n-prod
    restart: unless-stopped
    networks:
      - coolify
    labels:
      - caddy=n8n.yourdomain.com
      - caddy.reverse_proxy={{upstreams 5678}}
    environment:
      # Database configuration (use shared Supabase PostgreSQL)
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=db_prod
      - DB_POSTGRESDB_USER=postgres
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
      - DB_POSTGRESDB_DATABASE=postgres
      # Privacy settings
      - N8N_DIAGNOSTICS_ENABLED=false
      - N8N_PERSONALIZATION_ENABLED=false
      # Security keys
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}
      - N8N_USER_MANAGEMENT_JWT_SECRET=${N8N_USER_MANAGEMENT_JWT_SECRET}
      # Auto-generated webhook URL
      - WEBHOOK_URL=${N8N_PUBLIC_URL}
    volumes:
      - n8n_storage:/home/node/.n8n
      - ./n8n/data:/data
      - ./n8n/data/credentials:/data/credentials:ro
      - ./n8n/data/workflows:/data/workflows:ro
    depends_on:
      db_prod:
        condition: service_healthy
      n8n-import:
        condition: service_completed_successfully

  # ==========================================
  # WINDMILL WORKFLOW AUTOMATION PLATFORM
  # ==========================================

  # Windmill Database: Dedicated PostgreSQL instance for Windmill
  windmill-db-prod:
    image: postgres:16
    container_name: windmill-db-prod
    restart: unless-stopped
    networks:
      - coolify
    environment:
      - POSTGRES_USER=${SERVICE_USER_POSTGRES}
      - POSTGRES_PASSWORD=${SERVICE_PASSWORD_POSTGRES}
      - POSTGRES_DB=windmill-db
    volumes:
      - windmill_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 40s

  # Windmill Server: Web UI and API server
  windmill-server:
    image: ghcr.io/windmill-labs/windmill:main
    container_name: windmill-server-prod
    restart: unless-stopped
    networks:
      - coolify
    labels:
      - caddy=windmill.yourdomain.com
      - caddy.reverse_proxy={{upstreams 8000}}
    environment:
      - DATABASE_URL=postgres://${SERVICE_USER_POSTGRES}:${SERVICE_PASSWORD_POSTGRES}@windmill-db-prod:5432/windmill-db
      - MODE=server
      - BASE_URL=${WINDMILL_PUBLIC_URL}
    depends_on:
      windmill-db-prod:
        condition: service_healthy
    volumes:
      - windmill_worker_logs:/tmp/windmill/logs
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Windmill Worker 1: Default worker for job execution
  windmill-worker-1:
    image: ghcr.io/windmill-labs/windmill:main
    container_name: windmill-worker-1-prod
    restart: unless-stopped
    networks:
      - coolify
    environment:
      - DATABASE_URL=postgres://${SERVICE_USER_POSTGRES}:${SERVICE_PASSWORD_POSTGRES}@windmill-db-prod:5432/windmill-db
      - MODE=worker
      - WORKER_GROUP=default
    depends_on:
      windmill-db-prod:
        condition: service_healthy
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - windmill_worker_dependency_cache:/tmp/windmill/cache
      - windmill_worker_logs:/tmp/windmill/logs
    healthcheck:
      test: ["CMD-SHELL", "exit 0"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Optional additional workers - remove if not needed to free up resources
  # # Windmill Worker 2: Additional default worker for scaling
  # windmill-worker-2:
  #   image: ghcr.io/windmill-labs/windmill:main
  #   container_name: windmill-worker-2-prod
  #   restart: unless-stopped
  #   networks:
  #     - coolify
  #   environment:
  #     - DATABASE_URL=postgres://${SERVICE_USER_POSTGRES}:${SERVICE_PASSWORD_POSTGRES}@windmill-db-prod:5432/windmill-db
  #     - MODE=worker
  #     - WORKER_GROUP=default
  #   depends_on:
  #     windmill-db-prod:
  #       condition: service_healthy
  #   volumes:
  #     - /var/run/docker.sock:/var/run/docker.sock
  #     - windmill_worker_dependency_cache:/tmp/windmill/cache
  #     - windmill_worker_logs:/tmp/windmill/logs
  #   healthcheck:
  #     test: ["CMD-SHELL", "exit 0"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  # # Windmill Worker Native: Specialized native worker for high-performance execution
  # windmill-worker-native:
  #   image: ghcr.io/windmill-labs/windmill:main
  #   container_name: windmill-worker-native-prod
  #   restart: unless-stopped
  #   networks:
  #     - coolify
  #   environment:
  #     - DATABASE_URL=postgres://${SERVICE_USER_POSTGRES}:${SERVICE_PASSWORD_POSTGRES}@windmill-db-prod:5432/windmill-db
  #     - MODE=worker
  #     - WORKER_GROUP=native
  #     - NUM_WORKERS=8
  #     - SLEEP_QUEUE=200
  #   depends_on:
  #     windmill-db-prod:
  #       condition: service_healthy
  #   volumes:
  #     - windmill_worker_logs:/tmp/windmill/logs
  #   healthcheck:
  #     test: ["CMD-SHELL", "exit 0"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  # Windmill LSP: Language Server Protocol for code intelligence
  windmill-lsp:
    image: ghcr.io/windmill-labs/windmill-lsp:latest
    container_name: windmill-lsp-prod
    restart: unless-stopped
    networks:
      - coolify
    volumes:
      - windmill_lsp_cache:/root/.cache
    healthcheck:
      test: ["CMD-SHELL", "exit 0"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s 
